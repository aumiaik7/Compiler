
\documentclass[paper=letter, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{enumitem}

\usepackage{sectsty} % Allows customizing section commands

%these two packages are needed for xfig 
\usepackage[dvips,final]{graphicx}
\usepackage{color} %include even if images arenâ€™t in color

\usepackage{float}
%\floatstyle{boxed} 
\usepackage[export]{adjustbox}
\restylefloat{figure}

\usepackage{listings}
\lstset{
  basicstyle=\footnotesize\ttfamily,breaklines=true,      % the size of the fonts that are used for the code
  commentstyle=\color{mygreen},    % comment style
}

\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{\thepage} % Empty center footer
\fancyfoot[R]{} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

%\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
%\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
%\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{University of Lethbridge} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Technical Documentation of Compiler construction (Upto phase-II)\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{} % Your name

%\date{\normalsize\today} % Today's date or a custom date
\begin{document}
\maketitle
\newpage



{\bf Syntax Analysis:}
Syntax analysis is the second phase of compiler construction. Lexical analyzer processes stream of individual characters to token. Syntax analyzer or parser takes stream of tokens as input and with the help of Context Free Grammer (CFG) parse the stream of tokens we get from source file. Context free grammar is made up of four components:

$\bullet$ A set of non-terminals : Generate grammar.\\
$\bullet$ A set of terminals : Basic symbols or set of tokens.\\
$\bullet$ A set of productions : Grammars which consists of terminals and non-teminals. left side of a production is always non-terminal and right side is sequence of terminals and/or non-terminal that can be deduced form the left side.\\
$\bullet$ One non-terminal which is the start symbol from where the production starts.\\


{\bf Parser construction detail:}\\

Based on major functionality, parser construction can be divided into three parts.
\begin{enumerate}  
\item Recursive descent parsing 
\item First and Follow set computation
\item Error Recovery
\end{enumerate}

Before jumping into the parser we want to mention one thing that we changed in previous project file. There are three types of identifiers in the CFG of our PL but previously we had only one type of symbol ID for identifiers. Now, in parser an ID can be of variable name type or constatnt name type or procedure name type. If we don't resolve this problem then some productions become ambiguous for example, factor = constant | variableAccess. First of constant contains constantName and first of variableAccess contains variableName. So when we are at factor we must decide which production will we use next. So we must be able to differentiate. That's why we added an attribute for ID symbols which is idType.\\

$\bullet$ If idType = 1 then it is a variableName.\\ 
$\bullet$ If idType = 2	then it's a constantName.It occurs after CONST symbol.\\
$\bullet$ If idType = 3 then it's a procedureName.It occurs after PROC symbol. \\ 

When these cases occurs we set the idType of those id in symbol table.\\


{\bf Steps of syntax analysis:}\\
\underline {\bf a) Recursive descent parsing:}\\

$\bullet$ Associated Files: parser.cc, parser.h\\ 

The main portion of Recursive Descent Parser is implemented with the help of these files. parser.h is the interface of the parse class and we implemented the methods inside parser.cc.\\

The implementation of grammar rules are quite self explanatory. Our parser is recursive descent parser. For every non-terminal we have a method. We implemented the PL grammar from Brinch hansen book's Appendix B section. The grammar is ready for LL(1) parsing because for any look ahead token there is at most one production. We didn't find any case in which the grammar is ambiguous (as we somehow resolved the case identifier issue). Here is an example how we implemented the recursive descent parser. We have a non-terminal block and the method for this non-terminal is block(). The production rule for this non-terminal is :\\

block = 'begin' definitionPart statementPart 'end'.\\

Here 'begin' and 'end' are terminal symbols and definitionPart, statementPart are non-terminals. When we expect a terminal we match the symbol. If it does not match then we show corresponding error message. For non-terminals, we call the corresponding method. The above mentioned production rule is implemented as follows.

\begin{lstlisting}
void Parser::block()
	{
		outFile<<"block()"<<endl;
		if(!match(BEGIN))
		{
			cerr<<"Syntax Error: "<<"begin is missing at line "<< lineNo  <<endl;
			ErrorCount();
			syntaxError(1);
		}
			

		lookAheadToken();
		if(ff.firstOfDefinition(nextTok))
		{	
			definitionPart();
			statementPart();
		}
		else if(ff.followOfDefPart(nextTok))
		{
			statementPart();
		}
		else
		{
			//error message
			cerr<<"Syntax Error: "<<"Illegal start of definition/statement at line "<< lineNo <<endl;
			ErrorCount();
		}

		//this portion matches end and if not matched the shows corresponding error message
		if(!match(END))
		{
			//error message		
			cerr<<"Syntax Error: "<<"end is missing at line "<< lineNo <<endl;
			ErrorCount();
			syntaxError(2);
		}	
	
	}  	  	 
\end{lstlisting}

In this code block first we match 'begin' symbol, if not matched then show corresponding error message. definitionPart can be empty so we need to grab a token in advance to see wheather it is in first of definitionPart or in follow of definitionPart. If it is in first of definitionPart. Then we call definitionPart() and statementPart() and finally match 'end' symbol. The whole parser is implemented in same design. \\

\underline {\bf b) First and Follow set computation:}\\

$\bullet$ Associated Files: firstfollow.cc, firstfollow.h\\
Without computing first and follow set it is not possible to implement an LL(1) parser. Some productions produces several grammar rule, some productions produces emty strings. In these case we need first and follow sets. We can go to at most one state. For example:\\

definition = constantDefinition $\mid$ variableDefinition $\mid$ procedureDefinition\\

In this case, when we are in definition we grab a look ahead token and for this look ahead token we must decide which method should we choose. We have firstOfConstDef(Token), firstOfVariDef(Token) and firstOfProcDef(Token) methods in which we pass our look ahead token. method calls the  firstof nonterminal method or match the symbol. The methods returns either true or false. If the passed symbol is in first set then it returns true otherwise false. In this scheme we choose which method to call using look ahead token.\\

In our PL grammar two productions produces empty in right hand side. We need to compute follow only for these two non-terminals. They are definitionPart and statementPart. As guardedCommand = expression '$->$' statementPart, so follow of guardedCommand is also included in follow set of statement part. We have three method for computing follow sets:  followOfDefPart(Token), followOfStatePart(Token),followOfGuardedCommand(Token). The implementation of first and follow sets methods are nothing fancy. They actually match the passed symbol at the end. Different methods were used so that we could easily handle and reuse the follow sets.\\

\pagebreak
\underline {\bf c) Error Recovery:}\\
$\bullet$ Associated Files: parser.cc, parser.h\\ 

Error recovery is an important part of syntax analysis. As our parser is recursive descent parser if there is no error recovery the parser will exit from the program after encountering any error. But our intention is to find as many syntax error as possible. In chapter 5 of Brinch Hansen's text there are several algorithms for recovering error. First one is not good at all the one I mentioned above. Second one is good but not good enough. It's quite crude. We will have some fixed no. of stop symbol and when any error will occur we will start to read tokens until we have any of the stop symbols then will start parsing from that sate. It's good but crude as well because we may miss many errors in between. The third one is the best among these. For every terminal there are different sets of stop symbols that may occur after the terminal symbol. So when we try to match any terminal but fail, then we call syntaxError(int) method with corresponding code.

{\bf syntaxError(int) method:}\\
This method is used for error recovery. It is implemented using switch case structure. When we try to match any terminal symbol and fail then we call this method and pass corresponding code. Based on the code, the code block of corresponding case is executed. In the case, we continue to grab tokens until we reach to any of the stop symbols for the symbol which occured error. EOF is always in stop symbol set as we must stop at eof otherwise the parser will fall in loop.\\

We tried to recover from as many errors as possible but sometimes the parser might not recover properly. It may not recover if the structure of program is very bad. It will still show some errors but will not be able to parse through last line. But in most of the cases the error recovery scheme we implemented is good enough to recover from error and parse through the last line of the source file.\\


{\bf Lexical Analysis:}\\
Lexical analysis or scanning is the process where the stream of characters making up the source program is read from left-to-right and grouped into tokens. Lexical analyzer, which is also known as scanner development is the first phase of compiler design. The purpose of scanner is to prcoess stream of individual characters to tokens which will be used in the next phase of the compiler. In this project we have developed a scanner that is actually a finite state machine that continues to read input stream until it reaches to the end of file and produces a list of tokens from the source file.\\
For building a scanner we divided our project into 4 parts. They are as follows:\\
$\bullet$ Driver\\
$\bullet$ Administrator\\
$\bullet$ Scanner\\
$\bullet$ Hash table for storing word-tokens\\

Brief description about the parts mentioned above is given below:\\
$\bullet$ Driver:\\

Associated file: plc.cc\\
This is the part that drives the program. It allows the user to call the scanner from command line. When user runs the executable it actually calls the main function inside plc.cc. It also show the message whether scanning is done successfully or not afterwards.

$\bullet$ Administrator:\\

Associated files: administration.h, administration.cc\\
This part performs the tasks that are not directly related to compiler phases.\\

\underline {administration.h and administration.cc}: administration.h is the header file for Administration class which consists of the declaration of the private and public variables and methods. Implementation of the class is done inside administration.cc.\\

\underline {scan() method}:  The most important method inside Administration class in scan() method. From this method we continue to call method nextToken() of Scanner class until we reach to the end of file or we reach maximum number of errors. In each iteration, we can get any one of the following:
	i)   a valid token: In this case, the attributes of the token is stored in outFile. validTok(Symbol sym) method helps us to perform the task.
	ii)  new line : If we get a token for newline we don't store it, we simply increase the line counter which is needed for the purpose of scanner. NewLine() method helps us to perform the task.
	iii) an invalid token : When we get any invalid token, we write the corresponding error along with the line number in outFile.The same validTok(Symbol sym) method helps us to perform this task.


$\bullet$ Scanner:\\

Associated files: symbol.h,token.h, token.cc, scanner.h, scanner.cc.\\

\underline {symbol.h}: We made a list of the symbols of PL and defined the symbols as enum Symbol type in this file.The symbols defined inside this file are as follows:\\

ID=256, NUMERAL, BADNUMERAL, BADNAME, //256-259\\ 
	BADSYMBOL, BADCHAR, NEWLINE, NONAME, //260-263\\
	ENDOFFILE, DOT, COMMA, SEMICOLON, //264-267\\
	LEFTBRACKET,RIGHTBRACKET, AND, OR, //268-271\\
	NOT,LESST, EQUAL, GREATERT, //272-275\\
	LTE, GTE, PLUS, MINUS, //276-279\\
	TIMES, DIV, MOD,LEFTP, //280-283\\
	RIGHTP, ASSIGN,	GC1,GC2, //284-287 \\
	BEGIN, END, CONST, ARRAY, //288-291  //keywords form here\\
	INT, BOOL, PROC, SKIP, //292-295\\
	READ, WRITE, CALL, IF, //296-299\\
	DO, FI, OD, FALSE, TRUE //300-304\\
We assigned an integer number for each symbol and store this number for each symbol type instead of it's name in string.\\

Now we will discuss about the Token class\\
\underline {token.h and token.cc}: token.h is the interface of token class.The implementation of the functions are inside the file token.cc.When we create any object of Token class, by defaul it creates a token of NONAME symbol type which can be done by it's default constructor.But if we want to create a token of any specific symbol then the constructor is executed and Symbol, value and lexeme for that token are stored. For storing value and lexeme we have a stucture named attVal in this class. 
The constructors are used to set attribute values for symbol.In order to get the attribute values we have three getter funcions, getSymbol(), getValue() and getLexeme().\\

The most important part of the scanner is implemented inside scanner.h and scanner.cc.\\
\underline {scanner.h and scanner.cc}: scanner.h is interface of Scanner class and we implemented the methods inside scanner.cc.\\

\underline {Token nextToken() method}:The main operation of scanning is done inside this method which we call from Administration class.The finite state machine that we mentioned about scanner is actually implemented inside this function.
At first we read a character from file and based on the character, we go to the next state of our finite state machine.The nested if else code blocks are used to implement this. Let us consider the following examples of how we implemented the finite machine to detect a valid or invalid token.\\

Example 1:Let us assume we have read a digit from source file. So it enters inside "else if(isdigit(ch))" code block. Inside this code block we continue to read digits until we reach to a valid or invalid ending. For this purpose we just look one character ahead rather than reading it from file. peek() method helps us in this purpose. When we reach to an end of a digit we check either the numeral is valid or not. If valid then we construct a token object of NUMERAL type with it's associated value (we set lexeme to be empty string as lexeme is not needed here) and return it. 
Example 2:This second example is about '$<$' and '$<=$' symbol. When our read character from file is '$<$' we enter inside "else if(isSpecial(ch))" which is in general for all special symbols of PL and inside this code block we enter inside "else if(ch == '$<$')". So we have read a character. Now our symbol can be only '$<$' or '$<=$', so we look at the next character using peek() method. If the next character is '$=$' then it's a '$<=$' symbol. We read the '$=$' as well from file and construct a token object of LTE (less than or equal to) type otherwise if the look ahead character is not '$=$' then we create a token object of LESST (less than) type. Values and lexemes are not important for special symbols so value is set to -1 and lexeme is set to empty string.\\

We used some other helper functions which are used by nextToken() method.\\

Keywords and Identifiers: The pattern for keyword and identifier is almost same. All keywords have same pattern as identifier by definition. So when an identifier/keyword like pattern is detected by the finite machine then we insert the lexeme in symbol table. If its an identifier then it returns 1 or if it's a keyword then it returns the Symbol value. Value attribute for these token are not important.-1 is set for those.\\

printSymTable() method: This method is optional. It is used to print Symbol Table in the terminal, if user wants to.\\

When we detect an error in readig a line the finite machine doesn't detect any new token rather it continues to read characters from file until the look ahead character is a newline character.\\

$\bullet$ Hash table for storing word-tokens: \\

Associated files: symboltable.h, symboltable.cc\\

We implemented a hashtable for storing the word-tokens. Word-tokens are either keyword or identifier. The pattern for keyword and identifier is same so we must seperate them from each other. This purpose is served by this part.\\
\underline {symboltable.h and symboltable.cc}: symboltable.h  is the interface for Symboltable class.The implementation is inside symboltable.cc.\\
We have 17 keywords in PL. At the beginning of the execution of program (before starting scanning process) we preload the symbol table "htable" with keywords. htable is a hash table.
We have methods for searching and inserting entries in this table.\\
\underline {hashfn(string)}: It returns an index for a lexeme.\\
\underline {search(string) method}: This method is used to  determine whether a lexeme is stored in the htable or not. 
insert() method: It is used to insert a keyword or identifier object into htable. At first it checks whether the lexeme is already stored in the htable or not. If it is already stored then it does not store it again rather returns 1 if it is an identifier or the Symbol if it's a keyword. We don't need to store lexemes for keywords in symbol table for comparing it with the incoming lexemes. We implemented this using a simple encoding scheme. We know that our htable is already preloaded with keywords, only Symbol enum values are stored for those in htable. So during the scanning process when we get a lexeme, how we can be sure whether it is an identifier or keyword? From hashfn(string) we get an index for the lexeme. If the position is empty then the lexeme is definitely an identifier. But if the position is occupied, the lexeme can be an identifier which we already declared before in the source file or it can be a keyword or it can be a new lexeme which returns the same index from hashfn(string).
First of all we need to detect whether it is a keyword or not. If the occupied position has a token object of Symbol value $>=$ 288 then the object in occupied position is a keyword for sure as we assigned enum symbol of keywords $>=$288. But how can we be sure that our lexeme is a keyword. Suppose the symbol value of occupied cell is 291. We substracted 288 from 291 which is 3. Now we compare our lexeme with the string at third position of keywords array. If they match then its a keyword. Cost of searching is O(1) here. If the symbol value is less than 288 then we compare the lexeme with the lexeme stored in that position. If it matches then it is an identifier we already stored. If none of the two cases occurs then we go to the next position and start doing the same thing.\\

The maximum capacity of our hash table is 307. So if it gets full program will show an error message and will exit.  \\




\end{document}
%\end{document}



